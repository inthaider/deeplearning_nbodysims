{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from math import isclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_array = np.load('pert_sech2_train_2000.npy')\n",
    "train_tensor = torch.Tensor(train_array)\n",
    "train_dataset = TensorDataset(train_tensor[0, :, :]/80, train_tensor[1, :, :]/80) # create your datset\n",
    "\n",
    "test_array = np.load('pert_sech2_test_500.npy')\n",
    "test_tensor = torch.Tensor(test_array)\n",
    "test_dataset = TensorDataset(test_tensor[0, :, :]/80, test_tensor[1, :, :]/80,) # create your datsetb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 5000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "print(len(train_dataset))\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # # Linear function 4: 100 --> 100\n",
    "        # self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # # Non-linearity 4\n",
    "        # self.relu4 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 5 (readout): 100 --> 10\n",
    "        self.fc5 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        # Linear function 3\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 3\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # Linear function 5 (readout)\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 60\n",
    "hidden_dim = 100\n",
    "output_dim = 60\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "train_predictions = np.empty((2, output_dim))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, true_outputs) in enumerate(train_loader):\n",
    "        \n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        inputs = Variable(inputs)\n",
    "        true_outputs = Variable(true_outputs)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        model_outputs = model(inputs)\n",
    "        \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(model_outputs, true_outputs)\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 1000 == 0:\n",
    "\n",
    "            # Calculate Train Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through train dataset\n",
    "            for inputs, true_outputs in train_loader:\n",
    "                inputs = Variable(inputs)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                model_outputs = model(inputs)\n",
    "\n",
    "                # Total number of true_outputs\n",
    "                total += true_outputs.nelement()\n",
    "\n",
    "                nppredicted = model_outputs.detach().numpy()\n",
    "                nptrue = true_outputs.numpy()\n",
    "                size = np.size(nptrue, 0)\n",
    "\n",
    "                for j in range(size):\n",
    "                    predicted = nppredicted[j,:]\n",
    "                    true = nptrue[j,:]\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (np.isclose(predicted, true, rtol=0.5)).sum()\n",
    "            \n",
    "            train_predictions[0, :] = true\n",
    "            train_predictions[1, :] = predicted\n",
    "\n",
    "            train_accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Calculate Test Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for inputs, true_outputs in test_loader:\n",
    "                inputs = Variable(inputs)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                model_outputs = model(inputs)\n",
    "\n",
    "                # Total number of true_outputs\n",
    "                total += true_outputs.nelement()\n",
    "\n",
    "                nppredicted = model_outputs.detach().numpy()\n",
    "                nptrue = true_outputs.numpy()\n",
    "                size = np.size(nptrue, 0)\n",
    "\n",
    "                for j in range(size):\n",
    "                    predicted = nppredicted[j,:]\n",
    "                    true = nptrue[j,:]\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (np.isclose(predicted, true, rtol=0.5)).sum()\n",
    "            \n",
    "            test_accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Train Accuracy: {}. Test Accuracy: {}'.format(iter, loss.data, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 8: SAVE THE DATA\n",
    "'''\n",
    "output_array = np.empty((0, output_dim))\n",
    "target_array = np.empty((0, output_dim))\n",
    "# Iterate through test dataset\n",
    "for inputs, true_outputs in test_loader:\n",
    "    inputs = Variable(inputs)\n",
    "    \n",
    "    # Forward pass only to get logits/output\n",
    "    model_outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(model_outputs, true_outputs)\n",
    "    \n",
    "    npoutputs = model_outputs.detach().numpy()\n",
    "    output_array = np.concatenate((output_array, npoutputs))\n",
    "\n",
    "    nptargets = true_outputs.detach().numpy()\n",
    "    target_array = np.concatenate((target_array, nptargets))\n",
    "\n",
    "output_array\n",
    "np.save('pert_sech2_test_outputs', output_array)\n",
    "\n",
    "target_array\n",
    "np.save('pert_sech2_test_targets', target_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnsim",
   "language": "python",
   "name": "learnsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
